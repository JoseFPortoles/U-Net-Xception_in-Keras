{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "u-net-xception-style-cityscapes-segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoseFPortoles/U-Net-Xception_in-Keras/blob/master/u-net_cityscapes_car_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHG2l-l2o0_W",
        "colab_type": "text"
      },
      "source": [
        "# Car segmentation with the Cityscapes dataset\n",
        "[Cityscapes](https://www.cityscapes-dataset.com/) [1] is a dataset for semantic understanding of urban scenes. Among other data it contains 5000 color images with their corresponding finely  pixel-wise annotated ground-truth masks. The masks annotate 30 different categories of urban scenery objects including different types of vehicles, people and urban constructions among others.\n",
        "\n",
        "The following is a simple attempt at training a deep learning model for semantic segmentation of a single category, namely cars, but it is easily adaptable to other categories (by changing the value of a single variable).\n",
        "\n",
        "The neural architecture employed for this example, a U-Net-Xception style CNN, is adapted from the [keras blog](https://keras.io/examples/vision/oxford_pets_image_segmentation/) [2] with minimal changes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3lYSJNaqNhz",
        "colab_type": "text"
      },
      "source": [
        "## Set variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "102f0Z1KqNh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Variables\n",
        "x_imsize = 160\n",
        "y_imsize = 160\n",
        "img_size = (x_imsize,y_imsize)\n",
        "num_classes = 2\n",
        "batch_size = 32\n",
        "epochs = 100"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5LL0QWvqNh9",
        "colab_type": "text"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jwR3yjk_qNh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26dc4393-997f-4cdf-d54b-3915a614cbd5"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 80, 80, 32)   896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 80, 80, 32)   128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 80, 80, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 80, 32)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d (SeparableConv (None, 80, 80, 64)   2400        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 80, 64)   256         separable_conv2d[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 80, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 80, 80, 64)   4736        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 80, 64)   256         separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 40, 40, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 40, 40, 64)   2112        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 40, 40, 64)   0           max_pooling2d[0][0]              \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 40, 40, 64)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 40, 40, 128)  8896        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 40, 40, 128)  512         separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 40, 40, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 40, 40, 128)  17664       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 40, 40, 128)  512         separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 20, 20, 128)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 20, 20, 128)  8320        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 20, 20, 128)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 20, 20, 128)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 20, 20, 256)  34176       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 20, 20, 256)  1024        separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 20, 20, 256)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 20, 20, 256)  68096       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 20, 20, 256)  1024        separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 256)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 10, 10, 256)  33024       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 10, 10, 256)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 10, 10, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 10, 10, 256)  590080      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 10, 10, 256)  1024        conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10, 10, 256)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 10, 10, 256)  590080      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 10, 10, 256)  1024        conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 20, 20, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 20, 20, 256)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 20, 20, 256)  65792       up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 20, 20, 256)  0           up_sampling2d[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 20, 20, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 20, 20, 128)  295040      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 20, 20, 128)  512         conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 20, 20, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 20, 20, 128)  147584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 20, 20, 128)  512         conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 40, 40, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 40, 40, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 40, 40, 128)  32896       up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 40, 40, 128)  0           up_sampling2d_2[0][0]            \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 40, 40, 128)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 40, 40, 64)   73792       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 40, 40, 64)   256         conv2d_transpose_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 40, 40, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 40, 40, 64)   36928       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 40, 40, 64)   256         conv2d_transpose_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 80, 80, 128)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 80, 80, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 80, 80, 64)   8256        up_sampling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 80, 80, 64)   0           up_sampling2d_4[0][0]            \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 80, 80, 64)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 80, 80, 32)   18464       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 80, 80, 32)   128         conv2d_transpose_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 80, 80, 32)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 80, 80, 32)   9248        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 80, 80, 32)   128         conv2d_transpose_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 160, 160, 64) 0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 160, 160, 32) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 160, 160, 32) 2080        up_sampling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 160, 160, 32) 0           up_sampling2d_6[0][0]            \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 160, 160, 2)  578         add_6[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 2,058,690\n",
            "Trainable params: 2,054,914\n",
            "Non-trainable params: 3,776\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ImH5g9CqNiH",
        "colab_type": "text"
      },
      "source": [
        "## Load a model\n",
        "Uncomment and use instead of the model building cell if you want to load a pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fgD-QL3EqNiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model_path ='/content/U-Net-Xception_Segmentation.h5'\n",
        "model = keras.models.load_model(model_path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOfcnQRp0uZv",
        "colab_type": "text"
      },
      "source": [
        "## Gather file paths \n",
        "The dataset keeps images and ground thruth masks separated in different folders, each containing subfolders for the training and the validation sets which in turn contain subfolders for the different cities. For the purpose of this example we don't care about distinction between cities, so the following script scans every subdirectory gathering a list of paths for all the different images and masks and returning four list, namely training and validation sets of images and the corresponding masks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "U1EkrAenqNiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# This directory variables are set to the locations of masks and images in the \n",
        "# Cityscapes dataset locations.\n",
        "dir_msk = '/content/gtFine'\n",
        "dir_img = '/content/leftImg8bit'\n",
        "\n",
        "def gatherPaths(dir_img, dir_msk):\n",
        "    \"\"\" Recursively gather mask files under path dir_msk.\n",
        "    Then build list of corresponding image paths in the same\n",
        "    order.\"\"\"\n",
        "    msk_files = []\n",
        "    img_files = []\n",
        "    for i, r in enumerate(os.walk(dir_msk)):\n",
        "        if i==0:\n",
        "            towns = r[1]\n",
        "        else:\n",
        "            town = towns[i-1]\n",
        "            files = r[2]\n",
        "            for file in files:\n",
        "                if 'labelIds' in file:\n",
        "                    msk_files.append(\n",
        "                        os.path.join(\n",
        "                            dir_msk,\n",
        "                            town,\n",
        "                            file\n",
        "                        )\n",
        "                    )\n",
        "                    img_files.append(\n",
        "                        os.path.join(\n",
        "                            dir_img,\n",
        "                            town,\n",
        "                            file.split('_gtFine_')[0]\n",
        "                        ) \n",
        "                        + '_leftImg8bit.png'\n",
        "                    )\n",
        "    return img_files, msk_files\n",
        "\n",
        "dir_img_train = os.path.join(dir_img, 'train')\n",
        "dir_msk_train = os.path.join(dir_msk, 'train')\n",
        "\n",
        "input_train_paths, target_train_paths = gatherPaths(dir_img_train, dir_msk_train) \n",
        "\n",
        "dir_img_val = os.path.join(dir_img, 'val')\n",
        "dir_msk_val = os.path.join(dir_msk, 'val')\n",
        "\n",
        "input_val_paths, target_val_paths = gatherPaths(dir_img_val, dir_msk_val) \n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpBcB_-_qNiT",
        "colab_type": "text"
      },
      "source": [
        "## Data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6tj9tJoZqNiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "class CityScapesSegData(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, num_classes, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.img_size = img_size\n",
        "        self.mask_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) corresponding to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        y = np.zeros((batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, (path_img, path_msk) in enumerate(zip(batch_input_img_paths, batch_target_img_paths)):\n",
        "            img = load_img(path_img, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "            msk = load_img(path_msk, target_size=self.mask_size, color_mode=\"grayscale\")\n",
        "            msk = np.expand_dims(msk, 2)\n",
        "            y[j] = msk.astype('uint8')\n",
        "\n",
        "        y = np.where(y==26, 1, 0)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# Instantiate data generators for each split\n",
        "\n",
        "train_gen = CityScapesSegData(\n",
        "    batch_size,\n",
        "    num_classes,\n",
        "    img_size,\n",
        "    input_train_paths, \n",
        "    target_train_paths, \n",
        ")\n",
        "\n",
        "val_gen = CityScapesSegData(\n",
        "    batch_size, \n",
        "    num_classes, \n",
        "    img_size, \n",
        "    input_val_paths, \n",
        "    target_val_paths, \n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsryaULyqNim",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vk9_1HbXqNin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3fbebbb-1f97-445b-d8fe-796e961ec966"
      },
      "source": [
        "# Compile model for training\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.metrics import CategoricalCrossentropy\n",
        "import numpy as np\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.RMSprop(lr=1e-5, momentum = 0.9), \n",
        "    loss=\"sparse_categorical_crossentropy\", \n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"/content/U-Net-Xception_Segmentation.h5\",\n",
        "        monitor = 'val_loss',\n",
        "        save_best_only=True,\n",
        "        ),\n",
        "    #keras.callbacks.EarlyStopping(\n",
        "    #    monitor = 'val_loss',\n",
        "    #   patience = 50,\n",
        "    #)\n",
        "]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "history = model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 333s 4s/step - loss: 0.3747 - val_loss: 0.4578\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.1322 - val_loss: 0.5882\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 331s 4s/step - loss: 0.1066 - val_loss: 0.6835\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0927 - val_loss: 0.6807\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0862 - val_loss: 0.6122\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0797 - val_loss: 0.3610\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 333s 4s/step - loss: 0.0722 - val_loss: 0.2042\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 331s 4s/step - loss: 0.0666 - val_loss: 0.1562\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0619 - val_loss: 0.1200\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0579 - val_loss: 0.1044\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0535 - val_loss: 0.0977\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0514 - val_loss: 0.1031\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0476 - val_loss: 0.1369\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0452 - val_loss: 0.1381\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0425 - val_loss: 0.1120\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0416 - val_loss: 0.1088\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0395 - val_loss: 0.1197\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0380 - val_loss: 0.1215\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 326s 4s/step - loss: 0.0361 - val_loss: 0.1462\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 325s 4s/step - loss: 0.0347 - val_loss: 0.1213\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 325s 4s/step - loss: 0.0343 - val_loss: 0.1268\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0327 - val_loss: 0.1204\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0318 - val_loss: 0.1240\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0308 - val_loss: 0.1286\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0306 - val_loss: 0.1293\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0300 - val_loss: 0.1371\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 327s 4s/step - loss: 0.0297 - val_loss: 0.1279\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 327s 4s/step - loss: 0.0283 - val_loss: 0.1280\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0278 - val_loss: 0.1340\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0272 - val_loss: 0.1410\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0270 - val_loss: 0.1422\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0265 - val_loss: 0.1352\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0262 - val_loss: 0.1402\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0254 - val_loss: 0.1431\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0257 - val_loss: 0.1320\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 327s 4s/step - loss: 0.0251 - val_loss: 0.1407\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 331s 4s/step - loss: 0.0250 - val_loss: 0.1505\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0248 - val_loss: 0.1366\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0244 - val_loss: 0.1392\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0238 - val_loss: 0.1445\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 331s 4s/step - loss: 0.0236 - val_loss: 0.1417\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0229 - val_loss: 0.1583\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0227 - val_loss: 0.1468\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 331s 4s/step - loss: 0.0228 - val_loss: 0.1646\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0226 - val_loss: 0.1468\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0224 - val_loss: 0.1451\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0218 - val_loss: 0.1518\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0211 - val_loss: 0.1560\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0206 - val_loss: 0.1505\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0203 - val_loss: 0.1679\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0201 - val_loss: 0.1589\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 327s 4s/step - loss: 0.0200 - val_loss: 0.1622\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 327s 4s/step - loss: 0.0199 - val_loss: 0.1640\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 327s 4s/step - loss: 0.0195 - val_loss: 0.1596\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 328s 4s/step - loss: 0.0192 - val_loss: 0.1639\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 329s 4s/step - loss: 0.0193 - val_loss: 0.1640\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 330s 4s/step - loss: 0.0195 - val_loss: 0.1676\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 338s 4s/step - loss: 0.0195 - val_loss: 0.1676\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 342s 4s/step - loss: 0.0190 - val_loss: 0.1587\n",
            "Epoch 60/100\n",
            "31/92 [=========>....................] - ETA: 3:07 - loss: 0.0182"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bba56e00e5f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train the model, doing validation at the end of each epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HzVhR5594FK",
        "colab_type": "text"
      },
      "source": [
        "## Visualise predictions on a few example images\n",
        "Use some images from the test subset in order to visualise the trained model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "W5syMCqJqNi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from imgaug import SegmentationMapOnImage\n",
        "\n",
        "test_dir = '/content/leftImg8bit/test/berlin/'\n",
        "jpg_pathlist = os.listdir(test_dir)\n",
        "jpg_pathlist = [os.path.join(test_dir,x) for x in jpg_pathlist if '.jpg' in x or '.jpeg' in x or '.png' in x]\n",
        "\n",
        "def showImgMsk(path, model):\n",
        "  img = load_img(path, target_size = (160,160), color_mode='rgb')\n",
        "  img = img_to_array(img, data_format='channels_last' ).astype('uint8')\n",
        "  plt.figure(figsize=(12,36))\n",
        "  plt.subplot(131)\n",
        "  plt.imshow(img)\n",
        "  img_batch = np.expand_dims(img, axis=0)\n",
        "  pred_categorical = model.predict(img_batch)\n",
        "  pred = np.argmax(pred_categorical, axis=3)\n",
        "  plt.subplot(132)\n",
        "  plt.imshow(pred[0])\n",
        "  segmap = SegmentationMapOnImage(pred_categorical[0], shape=img.shape)\n",
        "  segmap = segmap.draw_on_image(img)\n",
        "  plt.subplot(133)\n",
        "  plt.imshow(segmap)\n",
        "\n",
        "for path in jpg_pathlist:\n",
        "  showImgMsk(path,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYQwArX_paUD",
        "colab_type": "text"
      },
      "source": [
        "## Citations\n",
        "\n",
        "[1] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes Dataset for Semantic Urban Scene Understanding,” in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n",
        "\n",
        "[2] François Chollet, \"Image segmentation with a U-Net-like architecture\". The Keras Blog, Code Examples ([Link](https://keras.io/examples/vision/oxford_pets_image_segmentation/))."
      ]
    }
  ]
}